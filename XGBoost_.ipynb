{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import squarify # pip install squarify\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprimimos la notacion cientifica en los outputs\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "train_values = pd.read_csv('train_values.csv', index_col='building_id')\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')\n",
    "test_values = pd.read_csv('test_values.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom features\n",
    "\n",
    "Notamos que si utilizamos el csv que exportamos del notebook de feature engineering, por mas que utilicemos los mismos features, el modelo da un peor puntaje que cuando utilizamos el train_values.csv. Por lo tanto, esperando eliminar estos efectos negativos, calcularemos los features que vayamos a utilizar aqui mismo, copiando el codigo que hicimos en el notebook mencionado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use(x):\n",
    "    if not x['has_secondary_use']: # Es una casa\n",
    "        return 1\n",
    "    if x['has_secondary_use_agriculture']: # Es de agricultura\n",
    "        return 2\n",
    "    if x['has_secondary_use_hotel']: # Es un hotel\n",
    "        return 3\n",
    "    if x['has_secondary_use_rental']: # Es de alquiler\n",
    "        return 4\n",
    "    if x['has_secondary_use_institution']: # Es una institucion\n",
    "        return 5\n",
    "    if x['has_secondary_use_school']: # Es una escuela\n",
    "        return 6\n",
    "    if x['has_secondary_use_industry']: # Es una industria\n",
    "        return 7\n",
    "    if x['has_secondary_use_health_post']: # Es un puesto de salud\n",
    "        return 8\n",
    "    if x['has_secondary_use_gov_office']: # Es una oficina de gobierno\n",
    "        return 9\n",
    "    if x['has_secondary_use_use_police']: # Es una estacion de policias\n",
    "        return 10\n",
    "    if x['has_secondary_use_other']: # tiene otro uso\n",
    "        return 11\n",
    "    \n",
    "train_values['use'] = train_values.apply(lambda x: use(x), axis=1)\n",
    "test_values['use'] = test_values.apply(lambda x: use(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values['height per area'] = train_values.height_percentage / train_values.area_percentage\n",
    "test_values['height per area'] = test_values.height_percentage / test_values.area_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = train_values[['has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
    "                       'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone',\n",
    "                       'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick',\n",
    "                       'has_superstructure_timber', 'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
    "                       'has_superstructure_rc_engineered', 'has_superstructure_other']]\n",
    "train_values[\"cant_materials\"] = subset.sum(axis=1)\n",
    "\n",
    "subset_test = test_values[['has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
    "                       'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone',\n",
    "                       'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick',\n",
    "                       'has_superstructure_timber', 'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
    "                       'has_superstructure_rc_engineered', 'has_superstructure_other']]\n",
    "test_values[\"cant_materials\"] = subset_test.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viendo esto podemos agregar otra columna que asigne 1 si tiene la cantidad pisos que resultÃ³ con daÃ±o considerable\n",
    "def bad_cant_floor(x):\n",
    "    if (x > 0 and x < 6) or x == 8:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train_values['bad_cant_floor'] = train_values.apply(lambda x: bad_cant_floor(x['count_floors_pre_eq']), axis=1)\n",
    "test_values['bad_cant_floor'] = test_values.apply(lambda x: bad_cant_floor(x['count_floors_pre_eq']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,y1 = train_values[['foundation_type', 'area_percentage', 'height_percentage', 'count_floors_pre_eq',\n",
    "                     'land_surface_condition', 'has_superstructure_cement_mortar_stone', 'age', 'geo_level_1_id',\n",
    "                     'geo_level_2_id','geo_level_3_id', 'height per area', 'cant_materials','bad_cant_floor']],train_labels['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.get_dummies(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.25, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clas = XGBClassifier(colsample_bytree = 0.4, \n",
    "                        learning_rate = 0.36,\n",
    "                        max_depth = 5, \n",
    "                        alpha = 1,\n",
    "                        n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:22:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.36, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=1,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clas.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = xg_clas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7140489017820141"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, predictions_train, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sin split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['foundation_type', 'area_percentage', 'height_percentage', 'count_floors_pre_eq',\n",
    "                     'land_surface_condition', 'has_superstructure_cement_mortar_stone', 'age', 'geo_level_1_id',\n",
    "                     'geo_level_2_id','geo_level_3_id', 'height per area', 'cant_materials', 'bad_cant_floor']\n",
    "\n",
    "train_values_subset = train_values[selected_features]\n",
    "test_values_subset = test_values[selected_features]\n",
    "\n",
    "train_values_subset = pd.get_dummies(train_values_subset)\n",
    "test_values_subset = pd.get_dummies(test_values_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clas = XGBClassifier(colsample_bytree = 0.4, \n",
    "                        learning_rate = 0.36,\n",
    "                        max_depth = 5, \n",
    "                        alpha = 1,\n",
    "                        n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.36, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=1,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clas.fit(train_values_subset,train_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = xg_clas.predict(train_values_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7194792038403536"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(train_labels, predictions_train, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
