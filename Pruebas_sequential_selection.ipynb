{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprimimos la notacion cientifica en los outputs\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')\n",
    "train_values = pd.read_csv('train_engineered_features.csv', index_col='building_id')\n",
    "test_values = pd.read_csv('test_engineered_features.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "train_values['position_code'] = ord_enc.fit_transform(train_values[['position']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = DATA , Y = TARGET \n",
    "X1, y1 =  train_values[['has_superstructure_timber','has_superstructure_bamboo',\n",
    "                              'has_superstructure_rc_engineered','has_secondary_use_rental',\n",
    "                              'has_secondary_use_institution','has_secondary_use_school',\n",
    "                              'has_secondary_use_industry','has_secondary_use_health_post',\n",
    "                              'has_secondary_use_gov_office','has_secondary_use_use_police',\n",
    "                              'has_secondary_use_other','more_families_than_mean','most_damaged_region',\n",
    "                              'has_most_damaged_construct_combination','position_code']], train_labels['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividimos el set de train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model_name = 'KNN'\n",
    "neigh = KNeighborsClassifier(n_neighbors=50,weights='distance')\n",
    "neigh.fit(X_train,y_train)\n",
    "predicciones_train = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5594198078817104"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, predicciones_train, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprimimos la notacion cientifica en los outputs\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')\n",
    "train_values = pd.read_csv('train_engineered_features.csv', index_col='building_id')\n",
    "test_values = pd.read_csv('test_engineered_features.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "train_values['position_code'] = ord_enc.fit_transform(train_values[['position']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = DATA , Y = TARGET \n",
    "X1, y1 =  train_values[['has_superstructure_timber','has_superstructure_bamboo',\n",
    "                              'has_superstructure_rc_engineered','has_secondary_use_rental',\n",
    "                              'has_secondary_use_institution','has_secondary_use_school',\n",
    "                              'has_secondary_use_industry','has_secondary_use_health_post',\n",
    "                              'has_secondary_use_gov_office','has_secondary_use_use_police',\n",
    "                              'has_secondary_use_other','more_families_than_mean','most_damaged_region',\n",
    "                              'has_most_damaged_construct_combination','position_code']], train_labels['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividimos el set de train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5679129200189305"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, predictions_train, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprimimos la notacion cientifica en los outputs\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')\n",
    "train_values = pd.read_csv('train_engineered_features.csv', index_col='building_id')\n",
    "test_values = pd.read_csv('test_engineered_features.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "train_values['position_code'] = ord_enc.fit_transform(train_values[['position']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = DATA , Y = TARGET \n",
    "X1, y1 =  train_values[['has_superstructure_timber','has_superstructure_bamboo',\n",
    "                              'has_superstructure_rc_engineered','has_secondary_use_rental',\n",
    "                              'has_secondary_use_institution','has_secondary_use_school',\n",
    "                              'has_secondary_use_industry','has_secondary_use_health_post',\n",
    "                              'has_secondary_use_gov_office','has_secondary_use_use_police',\n",
    "                              'has_secondary_use_other','more_families_than_mean','most_damaged_region',\n",
    "                              'has_most_damaged_construct_combination','position_code']], train_labels['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividimos el set de train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clas = XGBClassifier(colsample_bytree = 0.4, \n",
    "                        learning_rate = 0.36,\n",
    "                        max_depth = 5, \n",
    "                        alpha = 1,\n",
    "                        n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:06:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.36, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=1,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clas.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = xg_clas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5684373441117407"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, predictions_train, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprimimos la notacion cientifica en los outputs\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')\n",
    "train_values = pd.read_csv('train_engineered_features.csv', index_col='building_id')\n",
    "test_values = pd.read_csv('test_engineered_features.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values[\"position\"] = test_values[\"position\"].astype(\"category\")\n",
    "train_values[\"position\"]= train_values[\"position\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = DATA , Y = TARGET \n",
    "X1, y1 =  train_values[['has_superstructure_timber','has_superstructure_bamboo',\n",
    "                              'has_superstructure_rc_engineered','has_secondary_use_rental',\n",
    "                              'has_secondary_use_institution','has_secondary_use_school',\n",
    "                              'has_secondary_use_industry','has_secondary_use_health_post',\n",
    "                              'has_secondary_use_gov_office','has_secondary_use_use_police',\n",
    "                              'has_secondary_use_other','more_families_than_mean','most_damaged_region',\n",
    "                              'has_most_damaged_construct_combination','position']], train_labels['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature = [0,1],)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui intentamos ponerle como eval_metric el f1 score pero no logramos cambiar el multi_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos como técnica de boosting: gbdt (gradient boosted decision trees).\n",
    "\n",
    "lgb_clas = lgb.LGBMClassifier(boosting_type = 'gbdt', \n",
    "                              early_stopping_rounds = 3,\n",
    "                              n_jobs=4,\n",
    "                              colsample_bytree=0.75,\n",
    "                              learning_rate=0.22,\n",
    "                              max_depth=5,\n",
    "                              min_child_weight=5,\n",
    "                              n_estimators=91,\n",
    "                              scale_pos_weight=4,\n",
    "                              subsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=3, early_stopping_rounds=3 will be ignored. Current value: early_stopping_round=3\n",
      "[1]\ttraining's multi_logloss: 0.912647\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "[2]\ttraining's multi_logloss: 0.912582\n",
      "[3]\ttraining's multi_logloss: 0.912544\n",
      "[4]\ttraining's multi_logloss: 0.912513\n",
      "[5]\ttraining's multi_logloss: 0.912486\n",
      "[6]\ttraining's multi_logloss: 0.912466\n",
      "[7]\ttraining's multi_logloss: 0.912446\n",
      "[8]\ttraining's multi_logloss: 0.912433\n",
      "[9]\ttraining's multi_logloss: 0.912415\n",
      "[10]\ttraining's multi_logloss: 0.912387\n",
      "[11]\ttraining's multi_logloss: 0.912371\n",
      "[12]\ttraining's multi_logloss: 0.912352\n",
      "[13]\ttraining's multi_logloss: 0.912345\n",
      "[14]\ttraining's multi_logloss: 0.912332\n",
      "[15]\ttraining's multi_logloss: 0.912322\n",
      "[16]\ttraining's multi_logloss: 0.912316\n",
      "[17]\ttraining's multi_logloss: 0.912311\n",
      "[18]\ttraining's multi_logloss: 0.912306\n",
      "[19]\ttraining's multi_logloss: 0.9123\n",
      "[20]\ttraining's multi_logloss: 0.912295\n",
      "[21]\ttraining's multi_logloss: 0.912288\n",
      "[22]\ttraining's multi_logloss: 0.912281\n",
      "[23]\ttraining's multi_logloss: 0.912273\n",
      "[24]\ttraining's multi_logloss: 0.912263\n",
      "[25]\ttraining's multi_logloss: 0.912254\n",
      "[26]\ttraining's multi_logloss: 0.912249\n",
      "[27]\ttraining's multi_logloss: 0.912246\n",
      "[28]\ttraining's multi_logloss: 0.912239\n",
      "[29]\ttraining's multi_logloss: 0.912237\n",
      "[30]\ttraining's multi_logloss: 0.912233\n",
      "[31]\ttraining's multi_logloss: 0.912227\n",
      "[32]\ttraining's multi_logloss: 0.912223\n",
      "[33]\ttraining's multi_logloss: 0.912218\n",
      "[34]\ttraining's multi_logloss: 0.912214\n",
      "[35]\ttraining's multi_logloss: 0.912211\n",
      "[36]\ttraining's multi_logloss: 0.912209\n",
      "[37]\ttraining's multi_logloss: 0.912202\n",
      "[38]\ttraining's multi_logloss: 0.912199\n",
      "[39]\ttraining's multi_logloss: 0.912195\n",
      "[40]\ttraining's multi_logloss: 0.912194\n",
      "[41]\ttraining's multi_logloss: 0.912191\n",
      "[42]\ttraining's multi_logloss: 0.91219\n",
      "[43]\ttraining's multi_logloss: 0.912189\n",
      "[44]\ttraining's multi_logloss: 0.912187\n",
      "[45]\ttraining's multi_logloss: 0.912183\n",
      "[46]\ttraining's multi_logloss: 0.912181\n",
      "[47]\ttraining's multi_logloss: 0.91218\n",
      "[48]\ttraining's multi_logloss: 0.912179\n",
      "[49]\ttraining's multi_logloss: 0.912176\n",
      "[50]\ttraining's multi_logloss: 0.912173\n",
      "[51]\ttraining's multi_logloss: 0.912169\n",
      "[52]\ttraining's multi_logloss: 0.912167\n",
      "[53]\ttraining's multi_logloss: 0.912166\n",
      "[54]\ttraining's multi_logloss: 0.912163\n",
      "[55]\ttraining's multi_logloss: 0.912161\n",
      "[56]\ttraining's multi_logloss: 0.912159\n",
      "[57]\ttraining's multi_logloss: 0.912155\n",
      "[58]\ttraining's multi_logloss: 0.912153\n",
      "[59]\ttraining's multi_logloss: 0.912152\n",
      "[60]\ttraining's multi_logloss: 0.91215\n",
      "[61]\ttraining's multi_logloss: 0.912149\n",
      "[62]\ttraining's multi_logloss: 0.912147\n",
      "[63]\ttraining's multi_logloss: 0.912146\n",
      "[64]\ttraining's multi_logloss: 0.912144\n",
      "[65]\ttraining's multi_logloss: 0.912142\n",
      "[66]\ttraining's multi_logloss: 0.91214\n",
      "[67]\ttraining's multi_logloss: 0.912139\n",
      "[68]\ttraining's multi_logloss: 0.912138\n",
      "[69]\ttraining's multi_logloss: 0.912136\n",
      "[70]\ttraining's multi_logloss: 0.912134\n",
      "[71]\ttraining's multi_logloss: 0.912133\n",
      "[72]\ttraining's multi_logloss: 0.912132\n",
      "[73]\ttraining's multi_logloss: 0.91213\n",
      "[74]\ttraining's multi_logloss: 0.912129\n",
      "[75]\ttraining's multi_logloss: 0.912126\n",
      "[76]\ttraining's multi_logloss: 0.912124\n",
      "[77]\ttraining's multi_logloss: 0.912123\n",
      "[78]\ttraining's multi_logloss: 0.912121\n",
      "[79]\ttraining's multi_logloss: 0.912118\n",
      "[80]\ttraining's multi_logloss: 0.912117\n",
      "[81]\ttraining's multi_logloss: 0.912115\n",
      "[82]\ttraining's multi_logloss: 0.912114\n",
      "[83]\ttraining's multi_logloss: 0.912113\n",
      "[84]\ttraining's multi_logloss: 0.912112\n",
      "[85]\ttraining's multi_logloss: 0.91211\n",
      "[86]\ttraining's multi_logloss: 0.91211\n",
      "[87]\ttraining's multi_logloss: 0.912108\n",
      "[88]\ttraining's multi_logloss: 0.912106\n",
      "[89]\ttraining's multi_logloss: 0.912104\n",
      "[90]\ttraining's multi_logloss: 0.912103\n",
      "[91]\ttraining's multi_logloss: 0.912101\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[91]\ttraining's multi_logloss: 0.912101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.75, early_stopping_rounds=3,\n",
       "               learning_rate=0.22, max_depth=5, min_child_weight=5,\n",
       "               n_estimators=91, n_jobs=4, scale_pos_weight=4, subsample=0.5)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clas.fit(X_train, y_train, eval_set=(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = lgb_clas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5685524615955283"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, predictions_train, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprimimos la notacion cientifica en los outputs\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')\n",
    "train_values = pd.read_csv('train_engineered_features.csv', index_col='building_id')\n",
    "test_values = pd.read_csv('test_engineered_features.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values[\"position\"] = test_values[\"position\"].astype(\"category\")\n",
    "train_values[\"position\"]= train_values[\"position\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = DATA , Y = TARGET \n",
    "X1, y1 =  train_values[['has_superstructure_timber','has_superstructure_bamboo',\n",
    "                              'has_superstructure_rc_engineered','has_secondary_use_rental',\n",
    "                              'has_secondary_use_institution','has_secondary_use_school',\n",
    "                              'has_secondary_use_industry','has_secondary_use_health_post',\n",
    "                              'has_secondary_use_gov_office','has_secondary_use_use_police',\n",
    "                              'has_secondary_use_other','more_families_than_mean','most_damaged_region',\n",
    "                              'has_most_damaged_construct_combination','position']], train_labels['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x175005491c0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(use_best_model = True,\n",
    "                           eval_metric = 'MultiClass')\n",
    "model.fit(X_train, y_train, cat_features=[14], eval_set = (X_test, y_test), silent = True, early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5686803699108479"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, predictions_train, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprimimos la notacion cientifica en los outputs\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv', index_col='building_id')\n",
    "train_values = pd.read_csv('train_engineered_features.csv', index_col='building_id')\n",
    "test_values = pd.read_csv('test_engineered_features.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "train_values['position_code'] = ord_enc.fit_transform(train_values[['position']])\n",
    "test_values['position_code'] = ord_enc.fit_transform(test_values[['position']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = DATA , Y = TARGET \n",
    "X1, y1 =  train_values[['has_superstructure_timber','has_superstructure_bamboo',\n",
    "                              'has_superstructure_rc_engineered','has_secondary_use_rental',\n",
    "                              'has_secondary_use_institution','has_secondary_use_school',\n",
    "                              'has_secondary_use_industry','has_secondary_use_health_post',\n",
    "                              'has_secondary_use_gov_office','has_secondary_use_use_police',\n",
    "                              'has_secondary_use_other','more_families_than_mean','most_damaged_region',\n",
    "                              'has_most_damaged_construct_combination','position_code']], train_labels['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:47:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:48:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:48:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:49:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:49:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:49:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('rf', RandomForestClassifier()),\n",
       "                               ('xgb',\n",
       "                                XGBClassifier(base_score=None, booster=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=None, gamma=None,\n",
       "                                              gpu_id=None,\n",
       "                                              importance_type='gain',\n",
       "                                              interaction_constraints=None,\n",
       "                                              learning_rate=None,\n",
       "                                              max_delta_step=None,\n",
       "                                              max_depth=None,\n",
       "                                              min_child_weight=None,\n",
       "                                              missing=nan,\n",
       "                                              monotone_constraints=None,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              num_parallel_tree=None,\n",
       "                                              random_state=None, reg_alpha=None,\n",
       "                                              reg_lambda=None,\n",
       "                                              scale_pos_weight=None,\n",
       "                                              subsample=None, tree_method=None,\n",
       "                                              validate_parameters=None,\n",
       "                                              verbosity=None))],\n",
       "                   final_estimator=LGBMClassifier())"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('xgb', XGBClassifier())\n",
    "]\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators = estimators, final_estimator = LGBMClassifier()\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_train = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5686803699108479"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, predictions_train, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
